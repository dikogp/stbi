{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"pip install Sastrawi","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import StemmerFactory class\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory# create stemmer\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()# stemming process\nsentence = 'Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan'\noutput   = stemmer.stem(sentence)\nprint(output)\n# ekonomi indonesia sedang dalam tumbuh yang banggaprint(stemmer.stem('Mereka meniru-nirukannya'))\n# mereka tiru\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kalimat = \"Berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\"\nlower_case = kalimat.lower()\nprint(lower_case)# output\n# berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah korea selatan, jepang, singapura, hong kong, dan finlandia.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re # impor modul regular expression\nkalimat = \"Berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\"\nhasil = re.sub(r\"\\d+\", \"\", kalimat)\nprint(hasil)# ouput\n# Berikut ini adalah negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef preprocess_text(document):\n        # Remove all the special characters\n        document = re.sub(r'\\W', ' ', str(document))\n\n        # remove all single characters\n        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n\n        # Remove single characters from the start\n        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n\n        # Substituting multiple spaces with single space\n        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n\n        # Removing prefixed 'b'\n        document = re.sub(r'^b\\s+', '', document)\n\n        # Converting to Lowercase\n        document = document.lower()\n\n        \n        return document","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preprocess_text(kalimat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kalimat = \"rumah idaman adalah rumah yang bersih\"\npisah = kalimat.split()\nprint(pisah)# output\n# ['rumah', 'idaman', 'adalah', 'rumah', 'yang', 'bersih']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install nltk","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# impor word_tokenize dari modul nltk\nimport nltk\nfrom nltk.tokenize import word_tokenize \n \nkalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online.\"\n \ntokens = nltk.tokenize.word_tokenize(kalimat)\nprint(tokens)# ouput \n# ['Andi', 'kerap', 'melakukan', 'transaksi', 'rutin', 'secara', 'daring', 'atau', 'online', '.']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nkalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\nkalimat = kalimat.translate(str.maketrans('','')).lower()\n \ntokens = nltk.tokenize.word_tokenize(kalimat)\nkemunculan = nltk.FreqDist(tokens)\nprint(kemunculan.most_common())# \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nkemunculan.plot(30,cumulative=False)\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TfidfVectorizer \n# CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nimport pandas as pd\n# set of documents\ntrain = ['The sky is blue.','The sun is bright.']\ntest = ['The sun in the sky is bright', 'We can see the shining sun, the bright sun.']\n# instantiate the vectorizer object\ncountvectorizer = CountVectorizer(analyzer= 'word', stop_words='english')\ntfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n# convert th documents into a matrix\ncount_wm = countvectorizer.fit_transform(train)\ntfidf_wm = tfidfvectorizer.fit_transform(train)\n#retrieve the terms found in the corpora\n# if we take same parameters on both Classes(CountVectorizer and TfidfVectorizer) , it will give same output of get_feature_names() methods)\n#count_tokens = tfidfvectorizer.get_feature_names() # no difference\ncount_tokens = countvectorizer.get_feature_names()\ntfidf_tokens = tfidfvectorizer.get_feature_names()\ndf_countvect = pd.DataFrame(data = count_wm.toarray(),index = ['Doc1','Doc2'],columns = count_tokens)\ndf_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = ['Doc1','Doc2'],columns = tfidf_tokens)\nprint(\"Count Vectorizer\\n\")\nprint(df_countvect)\nprint(\"\\nTD-IDF Vectorizer\\n\")\nprint(df_tfidfvect)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ncolumns = ['sent', 'class']\nrows = []\n\nrows = [['This is my book', 'stmt'], \n        ['They are novels', 'stmt'],\n        ['have you read this book', 'question'],\n        ['who is the author', 'question'],\n        ['what are the characters', 'question'],\n        ['This is how I bought the book', 'stmt'],\n        ['I like fictions', 'stmt'],\n        ['what is your favorite book', 'question'],\n       ['This is how I bought the book', 'stmt'],\n        ['I like fictions', 'stmt'],\n        ['what is your favorite book', 'question']]\n\ntraining_data = pd.DataFrame(rows, columns=columns)\ntraining_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data['label'] = training_data['class'].apply(lambda x: 0 if x=='stmt' else 1)\ntraining_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(training_data['sent'], training_data['label'], test_size = 0.3,random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX_train_cv = cv.fit_transform(X_train)\nX_test_cv = cv.transform(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler \nscaler = StandardScaler() \nscaler.fit(X_train_cv)\nX_train_cv = scaler.transform(X_train_cv) \nX_test_cv = scaler.transform(X_test_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"word_freq = pd.DataFrame(X_train_cv.toarray(), columns=cv.get_feature_names())\ntop_words_df = pd.DataFrame(word_freq.sum()).sort_values(0, ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nnaive_bayes = MultinomialNB()\nnaive_bayes.fit(X_train_cv, y_train)\npredictions = naive_bayes.predict(X_test_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score\nprint('Accuracy score: ', accuracy_score(y_test, predictions))\nprint('Precision score: ', precision_score(y_test, predictions))\nprint('Recall score: ', recall_score(y_test, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncm = confusion_matrix(y_test, predictions)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['question', 'stmt'], yticklabels=['question', 'stmt'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier \nclassifier = KNeighborsClassifier(n_neighbors=5) \nclassifier.fit(X_train_cv, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = classifier.predict(X_test_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix \nprint(confusion_matrix(y_test, y_pred)) \nprint(classification_report(y_test, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(X_train_cv,y_train)\n\nY_pred_lr = lr.predict(X_test_cv)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nscore_lr = round(accuracy_score(Y_pred_lr,y_test)*100,2)\n\nprint(\"The accuracy score achieved using Logistic Regression is: \"+str(score_lr)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import svm\n\nsv = svm.SVC(kernel='linear')\n\nsv.fit(X_train_cv, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_svm = sv.predict(X_test_cv)\nscore_svm = round(accuracy_score(Y_pred_svm,y_test)*100,2)\n\nprint(\"The accuracy score achieved using Linear SVM is: \"+str(score_svm)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_pred_svm.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nmax_accuracy = 0\n\n\nfor x in range(20):\n    dt = DecisionTreeClassifier(random_state=x)\n    dt.fit(X_train_cv,y_train)\n    Y_pred_dt = dt.predict(X_test_cv)\n    current_accuracy = round(accuracy_score(Y_pred_dt,y_test)*100,2)\n    if(current_accuracy>max_accuracy):\n        max_accuracy = current_accuracy\n        best_x = x\n        \nprint(max_accuracy)\nprint(best_x)\n\n\ndt = DecisionTreeClassifier(random_state=best_x)\ndt.fit(X_train_cv,y_train)\nY_pred_dt = dt.predict(X_test_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_dt = round(accuracy_score(Y_pred_dt,y_test)*100,2)\n\nprint(\"The accuracy score achieved using Decision Tree is: \"+str(score_dt)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nmax_accuracy = 0\nfor x in range(20):\n    rf = RandomForestClassifier(random_state=x)\n    rf.fit(X_train_cv,y_train)\n    Y_pred_rf = rf.predict(X_test_cv)\n    current_accuracy = round(accuracy_score(Y_pred_rf,y_test)*100,2)\n    if(current_accuracy>max_accuracy):\n        max_accuracy = current_accuracy\n        best_x = x\n        \nprint(max_accuracy)\nprint(best_x)\n\nrf = RandomForestClassifier(random_state=best_x)\nrf.fit(X_train_cv,y_train)\nY_pred_rf = rf.predict(X_test_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_rf = round(accuracy_score(Y_pred_rf,y_test)*100,2)\n\nprint(\"The accuracy score achieved using Decision Tree is: \"+str(score_rf)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\nxgb_model.fit(X_train_cv, y_train)\n\nY_pred_xgb = xgb_model.predict(X_test_cv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score_xgb = round(accuracy_score(Y_pred_xgb,y_test)*100,2)\n\nprint(\"The accuracy score achieved using XGBoost is: \"+str(score_xgb)+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = [score_lr,score_nb,score_svm,score_knn,score_dt,score_rf,score_xgb]\nalgorithms = [\"Logistic Regression\",\"Naive Bayes\",\"Support Vector Machine\",\"K-Nearest Neighbors\",\"Decision Tree\",\"Random Forest\",\"XGBoost\"]    \n\nfor i in range(len(algorithms)):\n    print(\"The accuracy score achieved using \"+algorithms[i]+\" is: \"+str(scores[i])+\" %\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={'figure.figsize':(15,7)})\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Accuracy score\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"klastering"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n    \n# for performing text clustering    \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\n# for providing the path\nimport os\nprint(os.listdir('../input/'))\n\n# for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\n\ncolumns = ['sent', 'class']\nrows = []\n\nrows = [['This is my book', 'stmt'], \n        ['They are novels', 'stmt'],\n        ['have you read this book', 'question'],\n        ['who is the author', 'question'],\n        ['what are the characters', 'question'],\n        ['This is how I bought the book', 'stmt'],\n        ['I like fictions', 'stmt'],\n        ['what is your favorite book', 'question'],\n       ['This is how I bought the book', 'stmt'],\n        ['I like fictions', 'stmt'],\n        ['what is your favorite book', 'question']]\n\ntraining_data = pd.DataFrame(rows, columns=columns)\ntraining_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data['length'] = training_data['sent'].apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"training_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (15, 7)\nsns.distplot(training_data['length'], color = 'purple')\nplt.title('The Distribution of Length over the Texts', fontsize = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wordcloud\n\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(background_color = 'lightcyan',\n                      width = 1200,\n                      height = 700).generate(str(training_data['sent']))\n\nplt.figure(figsize = (15, 10))\nplt.imshow(wordcloud)\nplt.title(\"WordCloud \", fontsize = 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\n\ncv = CountVectorizer()\nwords = cv.fit_transform(training_data['sent'])\nsum_words = words.sum(axis=0)\n\n\nwords_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\nfrequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n\ncolor = plt.cm.twilight(np.linspace(0, 1, 20))\nfrequency.head(20).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = color)\nplt.title(\"Most Frequently Occuring Words - Top 20\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of X :\", words.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true_k = 2\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\nmodel.fit(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Top terms per cluster:\")\n\norder_centroids = model.cluster_centers_.argsort()[:, ::-1]\nterms = cv.get_feature_names()\n\nfor i in range(true_k):\n    print(\"Cluster %d:\" % i),\n    for ind in order_centroids[i, :10]:\n        print(' %s' % terms[ind]),\n    print\n\nprint(\"\\n\")\nprint(\"Prediction\")\nY = cv.transform([\"you read this book\"])\nprediction = model.predict(Y)\nprint(\"Cluster number :\", prediction)\nY = cv.transform([\"what is favorite book\"])\nprediction = model.predict(Y)\nprint(\"Cluster number :\", prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\ndocuments = [\"This little kitty came to play when I was eating at a restaurant.\",\n             \"Merley has the best squooshy kitten belly.\",\n             \"Google Translate app is incredible.\",\n             \"If you open 100 tab in google you get a smiley face.\",\n             \"Best cat photo I've ever taken.\",\n             \"Climbing ninja cat.\",\n             \"Impressed with google map feedback.\",\n             \"Key promoter extension for Google Chrome.\"]\n\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(documents)\n\ntrue_k = 2\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\nmodel.fit(X)\n\nprint(\"Top terms per cluster:\")\norder_centroids = model.cluster_centers_.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names()\nfor i in range(true_k):\n    print(\"Cluster %d:\" % i),\n    for ind in order_centroids[i, :10]:\n        print(' %s' % terms[ind]),\n    print\n\nprint(\"\\n\")\nprint(\"Prediction\")\n\nY = vectorizer.transform([\"chrome browser to open.\"])\nprediction = model.predict(Y)\nprint(prediction)\n\nY = vectorizer.transform([\"My cat is hungry.\"])\nprediction = model.predict(Y)\nprint(prediction)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n\n\n\ndocuments = [\"This little kitty came to play when I was eating at a restaurant.\",\n             \"Merley has the best squooshy kitten belly.\",\n             \"Google Translate app is incredible.\",\n             \"If you open 100 tab in google you get a smiley face.\",\n             \"Best cat photo I've ever taken.\",\n             \"Climbing ninja cat.\",\n             \"Impressed with google map feedback.\",\n             \"Key promoter extension for Google Chrome.\"]\n\ntfidf = TfidfVectorizer(stop_words='english')\nX = tfidf.fit_transform(documents).todense()\n# transform the data matrix into pairwise distances list\ndist_array = pdist(X)\n# calculate hierarchy\n#Z = linkage(dist_array, 'ward')\n#plt.title(\"Ward\")\n#dendrogram(Z, labels=labels)\n\nimport scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\nplt.title('Dendrogram')\nplt.xlabel('Nomor Dokumen')\nplt.ylabel('Jarak Euclidean')\nplt.show()\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\ncluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')  \ncluster.fit_predict(X) \nprint(cluster.labels_)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Silhouette score analysis to find the ideal number of clusters for K-means clustering\n\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50,random_state= 100)\n    kmeans.fit(df_pca_final_data)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(df_pca_final_data, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"salah yang dibawah"},{"metadata":{"trusted":true},"cell_type":"code","source":"error = []\nfor i in range(1, 4): \n knn = KNeighborsClassifier(n_neighbors=i)\n knn.fit(X_train_cv, y_train)\n pred_i = knn.predict(X_test_cv)\n #error.append(np.mean(pred_i != y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}